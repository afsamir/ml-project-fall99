{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit0794469bf3b54f85b78d84ff6f1086a0",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<div dir='rtl'>\n",
    "در این قسمت به سراغ استفاده از روش بردار پشتیبان می‌رویم.\n",
    "<br>\n",
    "با توجه یه اینکه ویژگی‌های داده بیشتر از نوع دسته‌بندی هستند، انتظار زیادی از این روش روی این داده نمی‌رود.\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as ps\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3768416 entries, 0 to 3768415\nData columns (total 16 columns):\n #   Column        Dtype\n---  ------        -----\n 0   displayId     int64\n 1   timestamp     int64\n 2   dayOfWeek     int64\n 3   hourOfDay     int64\n 4   advertiserId  int64\n 5   campaignId    int64\n 6   creativeId    int64\n 7   publisher     int64\n 8   widgetId      int64\n 9   device        int64\n 10  os            int64\n 11  browser       int64\n 12  source        int64\n 13  docId         int64\n 14  userId        int64\n 15  clicked       int64\ndtypes: int64(16)\nmemory usage: 460.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = ps.read_csv('/content/gdrive/My Drive/train_data.csv' if IN_COLAB else 'train_data.csv')\n",
    "data.info()"
   ]
  },
  {
   "source": [
    "<div dir='rtl'>ابتدا به سراغ تقسیم‌بندی داده‌ها می‌رویم. با توجه به نامتوازن بودن داده‌ها، روش‌هایی مانند (undersampling) نمونه‌زایی (oversampling) و نمونه‌کاهی می‌تواند کمک کند اما به توجه به هزینه این روش‌ها که دور شدن از داده‌واقعی است، تلاش خود را بر اصلاح تابع خطا به شکلی که داده‌های کلاس نادر را مهمتر درنظر گیرد می‌گذاریم.\n",
    "<br>\n",
    "کتابخانه sklearn الگوریتم svm را به صورت وزن دار پیاده‌سازی کرده است. به طوری‌که میتوان با وزن‌دادن به کلاس‌ها، تشخیص درست‌ یک کلاس را بر دیگر کلاس‌ها اولویت داد. با توجه به این‌که تعداد نمایش‌های منجر به مراجعه، ۲۵ درصد کل داده است، وزن این کلاس راتعیین می‌کنیم.\n",
    "<div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsf = svm.SVC(kernel='linear', C=1, class_weight={0:.25, 1:.75})"
   ]
  },
  {
   "source": [
    "<div dir='rtl'>\n",
    "اکنون عملیات روی داده‌ها را شروع می‌کنیم.\n",
    "<br>\n",
    "اول ویژگی‌های غیر ترتیبی را حذف می‌کنیم و بعد داده‌ی یادگیری را از داده‌ آزمون جدا می‌کنیم.\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000 entries, 2994261 to 2371043\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype\n---  ------     --------------  -----\n 0   dayOfWeek  10000 non-null  int64\n 1   hourOfDay  10000 non-null  int64\n 2   clicked    10000 non-null  int64\ndtypes: int64(3)\nmemory usage: 312.5 KB\nNone\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(         dayOfWeek  hourOfDay  clicked\n",
       " 801704           6         15        0\n",
       " 99645            4          7        0\n",
       " 897792           0          5        0\n",
       " 487237           5          9        0\n",
       " 866664           6         22        1\n",
       " ...            ...        ...      ...\n",
       " 3592105          3         10        0\n",
       " 1928364          3          8        0\n",
       " 2197039          4          8        1\n",
       " 1312829          1          9        0\n",
       " 273906           4         17        0\n",
       " \n",
       " [8000 rows x 3 columns],\n",
       "          dayOfWeek  hourOfDay  clicked\n",
       " 1768653          2         18        0\n",
       " 3312677          2          9        0\n",
       " 2151268          4          6        0\n",
       " 465442           5          8        0\n",
       " 252135           4         15        1\n",
       " ...            ...        ...      ...\n",
       " 2902361          0         15        0\n",
       " 35638            4          5        0\n",
       " 2262462          4         12        0\n",
       " 1434366          1         18        1\n",
       " 2111783          4          3        0\n",
       " \n",
       " [2000 rows x 3 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = data.sample(10000)\n",
    "\n",
    "clean_data = data.drop(data.columns[[0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]] , axis=1)\n",
    "print(clean_data.info())\n",
    "train, test = train_test_split(clean_data, test_size=0.2)\n",
    "train, test"
   ]
  },
  {
   "source": [
    "<div dir=rtl'>\n",
    "حالا به سراغ خود الگوریتم می‌رویم.\n",
    "<br>\n",
    "\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clsf.fit(train[['dayOfWeek','hourOfDay']], train[['clicked']])\n",
    "real_y = test[['clicked']]\n",
    "pred_y = clsf.predict(test[['dayOfWeek','hourOfDay']])\n",
    "# (clsf.predict(newdata[0:2]), data[1:2].clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "در پایان با استفاده از متریک‌های مختلف، یادگیری انجام شده را می‌سنجیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC : 0.5000\nCross-entropy loss : 7.8403 \nF1 score : 0.7730\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"AUC : %.4f\" %metrics.roc_auc_score(real_y, pred_y))\n",
    "print(\"Cross-entropy loss : %.4f \" %metrics.log_loss(real_y,pred_y))\n",
    "print(\"F1 score : %.4f\" %metrics.f1_score(real_y,pred_y,average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div dir='rtl'>\n",
    "اکنون برای استفاده بهتر از داده‌ها سعی میکنیم فیچر‌ها را ترتیبی کنیم.\n",
    "<br>\n",
    "نحوه کار این است که نمره هر کلاس را تعداد مشاهده‌ی آن در داده‌های کلیک شده می‌گذاریم\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newdata=ps.DataFrame()\n",
    "\n",
    "clickedsize = len(data[data.clicked==1])\n",
    "def t(element):\n",
    "    # print(val[element])\n",
    "    try:\n",
    "        if val[element] > 0:\n",
    "            return val[element]/clickedsize\n",
    "        else: return mean\n",
    "    except Exception:\n",
    "        return mean\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in ['dayOfWeek', 'hourOfDay', 'clicked']:\n",
    "        newdata[col] = data[col]\n",
    "    else:\n",
    "        val = data[data.clicked==1][col].value_counts()\n",
    "        mean=np.mean(val)\n",
    "        newdata[col] = data[col].map(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtrain, newtest = train_test_split(newdata, test_size=0.2)\n",
    "\n",
    "newtrain\n",
    "\n",
    "clsf.fit(newtrain.drop(data.columns[[15]], axis=1), newtrain[['clicked']])\n",
    "newreal_y = newtest[['clicked']]\n",
    "\n",
    "newpred_y = clsf.predict(newtest.drop(data.columns[[15]], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC : 0.9994\nCross-entropy loss : 0.0345 \nF1 score : 0.9990\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC : %.4f\" %metrics.roc_auc_score(newreal_y, newpred_y))\n",
    "print(\"Cross-entropy loss : %.4f \" %metrics.log_loss(newreal_y,newpred_y))\n",
    "print(\"F1 score : %.4f\" %metrics.f1_score(newreal_y,newpred_y,average=\"micro\"))"
   ]
  }
 ]
}